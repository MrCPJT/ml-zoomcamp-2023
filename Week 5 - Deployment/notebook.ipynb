{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--- \n",
    "format:\n",
    "    html:\n",
    "        embed-resources: true\n",
    "        theme:\n",
    "            light: default\n",
    "            dark: [default, theme-dark.scss]\n",
    "        highlight-style: atom-one        \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Machine Learning Models\n",
    "\n",
    "After developing a ML model, we want to enable users to utilise the model without having to run any code. One means of doing this is through deploying the model to a server. Users can then make API calls and retrieve their requested data/predictions.\n",
    "\n",
    "We will practice model deployment using Churn Predicition Project from weeks 3 and 4 of ML Zoomcamp (2023). Let us first run some code to train and evaluate the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Validation framework\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate indices for K-Fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Feature matrix formatter (dictionary vectoriser)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Logistic regression (sigmoid ver. linear regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ROC curves and AUC score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "df = pd.read_csv(r'..\\Week 3\\telco-customer-churn\\WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# Cleaning column names\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Storing categorical variables\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "# Cleaning contents of categorical series\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Parsing totalcharges as numeric (falsely parsed as an object)\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "# Converting .churn yes/no outcomes to binary (1/0)\n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing validation framework using sklearn\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical series\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "# Categorical series\n",
    "categorical = [\n",
    "    'gender',\n",
    "    'seniorcitizen',\n",
    "    'partner',\n",
    "    'dependents',\n",
    "    'phoneservice',\n",
    "    'multiplelines',\n",
    "    'internetservice',\n",
    "    'onlinesecurity',\n",
    "    'onlinebackup',\n",
    "    'deviceprotection',\n",
    "    'techsupport',\n",
    "    'streamingtv',\n",
    "    'streamingmovies',\n",
    "    'contract',\n",
    "    'paperlessbilling',\n",
    "    'paymentmethod',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for training classifier model\n",
    "def train(df_train, y_train, C=1.0):\n",
    "\n",
    "    # Initialising sklearn dictionary vectoriser (sparse matrix == False)\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "\n",
    "    # Creating dictionary of training features\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    # Transforming dictionary to formatted feature matrix using DictVectoriser\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    # Initialising logistic regression model using sklearn\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "\n",
    "    # Training model on train split\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for making generating model predictions\n",
    "def predict(df, dv, model):\n",
    "    \n",
    "    # Creating dictionary of training features\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    # Transforming dictionary to formatted feature matrix using DictVectoriser\n",
    "    X = dv.transform(dicts)\n",
    "\n",
    "    # Obtaining probabilistic predictions for validation split\n",
    "    y_pred = model.predict_proba(X)[:,1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "C = 1.0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.841 +- 0.009\n"
     ]
    }
   ],
   "source": [
    "# Generating indices for n_splits shuffled splits\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "# Empty array to store scores\n",
    "scores = []\n",
    "\n",
    "# Indices for train and validation parts\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572386167896259"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the final model and validating on test split\n",
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(df_test.churn.values, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "\n",
    "Our model currently lives inside a notebook; therefore, we cannot deploy the model to a webservice. Instead we need to extract (save) the model (python code) first so that we can generate a webservice using python frameworks, like Flask, Django, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying output name\n",
    "output_file = f'model_C={C}.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file to edit (write bytes)\n",
    "f_out = open(output_file, 'wb')\n",
    "\n",
    "# Saving the trained model in output file\n",
    "pickle.dump((dv, model), f_out)\n",
    "\n",
    "# Closing the file - prevents accidently writing to file\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate, auto-closing method\n",
    "with open(output_file, 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "\n",
    "Having saved the model using pickle, we want to test whether the file was written correctly. By restarting the kernel here, one can run the code below, reading the output file and verifying whether the model was successfully extracted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying file to read\n",
    "input_file = 'model_C=1.0.bin'\n",
    "\n",
    "# Reading input file\n",
    "with open(input_file, 'rb') as f_in:\n",
    "    (dv, model) = pickle.load(f_in)\n",
    "\n",
    "# Testing whether model saved correctly or not\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model on Dummy Customer\n",
    "\n",
    "Having loaded the model, let us test the model's functionality on a dummy customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy customer profile\n",
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that customer will churn: 63.6%\n"
     ]
    }
   ],
   "source": [
    "# Formatting feature matrix using extracted dictionary vectorizer\n",
    "X = dv.transform([customer])\n",
    "\n",
    "# Using model for prediction\n",
    "customer_proba = model.predict_proba(X)[0,1]\n",
    "print('Probability that customer will churn: {0:.1f}%'.format(customer_proba*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Python Code\n",
    "\n",
    "To serve the model to users we must first extract the python code from this notebook. This results in .py file(s) which we can then serve using python frameworks, like Flask. Converting the notebook (.ipynb) to a Python script (.py) is not difficult, but some modifications can be made to make the script more accessible/easy-to-understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving the Model Using Flask\n",
    "\n",
    "Having a Python script for training the model we can enable users to make requests by creating a prediction script which we then serve using Flask. Flask converts Python code into a GET/PULL/... webservice. In our case, the user will want to input information and be returned a prediction of whether or not a user will churn; hence, the PULL method is most appropriate.\n",
    "\n",
    "Serving with Flask is not recommended outside of development; instead, we can use Waitress, a Python WSGI server, which is a windows-equivalent of Gunicorn for Linux. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment and Dependency Management\n",
    "\n",
    "Unique services live in seperate environments to each other. This can lead to version inconsistencies, which may cause conflicts down the line. Virtual environments seperate/isolate processeses preventing potential conflicts.\n",
    "\n",
    "Examples of virtual environment solutions:\n",
    "* venv\n",
    "* conda\n",
    "* pipenv\n",
    "* poetry\n",
    "\n",
    "For this project we will use pipenv. Using pipenv, we can initialise an environment and be particular about dependecy versions, etc. One can then use `pipenv shell` to run commands using the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker\n",
    "\n",
    "Docker enables us to isolate the application from other services/processes running on our local computer. Services are isolated in *containers* with their respective dependencies, software versions, etc. Using Docker enables us to compartmentalise our project which is useful for managing processes and workflow. For example, we may choose to upload one service to the cloud and this is straightforward if our service lives in its own compartment.\n",
    "\n",
    "#### 1. Write the Dockerfile\n",
    "\n",
    "#### 2. Build the Docker image\n",
    "    \n",
    "- Remembering to use the same Python version as pipfile<br>\n",
    "\n",
    "#### 3. Run the Docker image\n",
    "    \n",
    "- Remembering to expose container port to host machine\n",
    "\n",
    "\n",
    "Having creating a Docker instance, we can run the service inside the Docker container and access the service using an external script.\n",
    "\n",
    "### Cloud Deployment\n",
    "\n",
    "After containing the churn prediction service in a Docker container, there is no longer a need to host the container on a local machine. Instead, one can upload the container to a cloud service and access the prediction service locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
